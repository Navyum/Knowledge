---
title: 概念
date: 2024-11-04 11:48:00
author: Navyum
tags: 
 - LLM
 - 概念
categories: 
 - 笔记
---

## 概念

## 大模型和小模型：

## Transformer
* Transformer 是 Google 的团队在 2017 年提出的一种 NLP 经典模型，现在比较火热的 Bert 也是基于 Transformer。
* Transformer 模型使用了注意力机制（attention mechanisms），不采用 RNN 的顺序结构，使得模型可以并行化训练，而且能够拥有全局信息
* Transformer 使用的是Encoder + Decoder的结构

## Transformer 整体结构
* 添加了补充信息（手动修改版）：
![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/4c3e702e3e84a73026ad2f2ab2087d3d.png)


## Transformer的编码器（Encode Block）：
* 组成：由Multi-Head Attention，Add & Norm，Feed Forward 组成
* 作用：通过多头注意力机制、带有残差连接、层归一化、逐位置前馈网络，将输入的Embedding丰富化，形成更丰富的输入特征

### softmax函数：
* 作用：将一组输入通过函数的方式，预测每个输出的可能概率，这个转换函数即softmax函数
* 公式：
$$\sigma(z)_j=\frac{e^{z_j}}{\sum_{i = 1}^{n}e^{z_i}} \\
  z=[z_1,z_2,...,z_n] \\
  z:\text{输入向量}，n：\text{向量的维度}，\sigma(z)_j：\text{输出向量中，第j个元素的概率}$$

