---
title: 概念
date: 2024-11-04 11:48:00
author: Navyum
tags: 
 - LLM
 - 概念
categories: 
 - 笔记
---

## 概念

## 大模型和小模型：

## Transformer
Transformer 是 Google 的团队在 2017 年提出的一种 NLP 经典模型，现在比较火热的 Bert 也是基于 Transformer。Transformer 模型使用了 Self-Attention 机制，不采用 RNN 的顺序结构，使得模型可以并行化训练，而且能够拥有全局信息
Transformer 使用的是Encoder + Decoder的结构

## Transformer 整体结构
![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/a7041312d53af1e5e5f00d28ac53d8c1.png)

## Transformer的编码器（Encode Block）：
由：Multi-Head Attention，Add & Norm，Feed Forward 组成


### softmax函数：
* 作用：将一组输入通过函数的方式，预测每个输出的可能概率，这个转换函数即softmax函数
* 公式：
  $\sigma(z)_j=\frac{e^{z_j}}{\sum_{i = 1}^{n}e^{z_i}}$
  $z=[z_1,z_2,...,z_n]$：输入向量，$n$：向量的维度，$\sigma(z)_j$：输出向量中，第j个元素的概率

### Transformer的输入：

#### 单词的表示向量x：
x = 单词 Embedding + 位置 Embedding

#### 单词 Embedding：
预训练获取：
- Word2Vec
- Glove
- Transformer

#### 位置 Encoding（Position Encoding）：
* 表示单词出现在句子中的位置（因为 Transformer 不采用 RNN 的结构，而是使用全局信息，这样就不能利用单词的顺序信息）
* 位置Encoding与单词Embedding具有相同的维度$d_{model}$
* 位置PE的计算：
  $PE_{(pos,2i)}=\sin(pos/10000^{2i/d_{model}})$
  $PE_{(pos,2i + 1)}=\cos(pos/10000^{2i/d_{model}})$
  $pos$：单词位置， $i$：维度

### Transformer的注意力机制：
* 注意力函数可以描述为将一个查询$Q$和一组键值对$KV$映射到一个输出，其中查询$Q$、键$K$、值$V$和输出$Attention(Q,K,V)$都是向量。
* 输出$Attention(Q,K,V)$是值的加权和，其中分配给每个值的权重是由查询与相应键的相容函数计算得出的
* QKV的计算：
  ![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/47c59ff8851a868aeae84a0525c4da52.png)
  其中WQ、WK、WV是线性变换矩阵，通过X进行线性变换得到QKV

#### 缩放点积注意力（Scaled Dot-Product Attention）
* 是计算注意力权重的一种具体方法
* 通过计算查询向量和键向量的点积 $QK^{\mathrm{T}}$，然后将这个点积结果除以 $\sqrt{d_k}$（$d_k$是键向量的维度，即向量矩阵的列数）进行缩放，再经过 $Softmax$ 函数得到注意力权重。最后用这些权重对值向量 $V$ 进行加权求和得到输出：
  $Attention(Q,K,V)=softmax\left(\frac{QK^{\mathrm{T}}}{\sqrt{d_k}}\right)V$
* 缩放的目的：是为了避免当 ${d_k}$ 较大时，点积结果过大，导致 Softmax 函数的梯度变得极小，从而影响模型的训练。
* 结构示意图：
![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/0ee9b4498e271403002a700d767d4f5f.png)
* 计算过程图：
  ![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/392f6cc206b26013f4d85e924812c904.png)
  ![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/ff89d34afb9474041d5d2908c815d0b8.png)
  ![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/ef72aa4b3db0b54c5800d93cd0f21d05.png)

#### 多头注意力（Multi-Head Attention）
* 多头注意力是由多个缩放点积注意力组合形成的
* 多头注意力使模型能够在不同位置共同关注来自不同表示子空间的信息
* 由于每个头的维度降低，总计算成本与具有全维度的单头注意力相似
  $MultiHead(Q,K,V)=Concat(head_1,\ldots,head_h)W^{O}\quad where\ head_i = Attention(QW^{Q}_i,KW^{K}_i,VW^{V}_i)$
  一般 $h=8$ 表示8个注意头，且 $d_k = d_v = d_model/h = 64$
* 结构示意图：
  ![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/ff13402258673dd3ab171400e01e8c4a.png)
* 计算示意图：
  ![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/5312a68962d60c9f54bb0e1ec8e784aa.png)
  ![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/00ee0bbd4935869cefb44c098518a433.png)

#### 自注意力机制（self-Attention）

### Transformer的 残差连接与层归一化层（Add & Norm）：
#### Add&Norm
* LayerNorm主要是将上层的结果进行归一化（LayerNorm）处理
  公式：$\text{LayerNorm}(x + MultiHeadAttention(x))$、$\text{LayerNorm}(x + FeedForward(x))$
  X表示 Multi-Head Attention 或者 Feed Forward 的输入，MultiHeadAttention(X) 和 FeedForward(X) 表示对应的输出 (输出与输入 X 维度是一样的，所以可以相加)

* Add：
  Add指 X + MultiHeadAttention(X)，是一种残差连接。通常用于解决多层网络训练的问题，**可以让网络只关注当前差异的部分** (在 ResNet 中经常用到)
* Norm：
  Norm指 Layer Normalization(通常用于 RNN 结构)。将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛
* 残差连接公式：
  ![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/5bbee0d3908a62831dbe4f9a08f2e807.png)

### Transformer的 逐位置前馈网络（Feed-Forward Networks）：
* 每一层都包含一个全连接前馈网络，该网络分别且同等地应用于每个位置。
* 它由两个线性变换组成，中间有一个 ReLU 激活函数：
  $FFN(x)=\max(0,xW_1 + b_1)W_2 + b_2$
  $x$：输入，$W_1$、$W_2$：线性变换权重矩阵，$b_1$、$b_2$：偏置项
* ReLU 激活函数（Rectified Linear Unit）：
  $y=max(0,x)$，当输入x大于0时，激活函数

## Transformer的解码器（Decode Block）：
由：Masked Multi-Head Attention、Multi-Head Attention、Add&Norm、Feed-Forward、Softmax组成

### 解码器中的掩蔽多头注意力（Masked Multi-Head Attention）：
* 在 Decoder 的时候，是需要根据之前的已预测结果，求解当前位置最有可能的预测结果。因为有顺序的要求，所以需要逐个预测。通过Mask操作，可以避免当前位置知道后续位置的信息。
* Mask操作本身通过Mask矩阵进行变换得到结果
* 公式：$Attention(Q,K,V)=softmax\left(\frac{QK^{\mathrm{T}}Mask}{\sqrt{d_k}}\right)V$
* Mask预测过程：
    * 对$QK^{\mathrm{T}}$的结果应用Mask矩阵转换
      ![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/8e7c10689439156e04e4a9a349be3600.png)

### 解码器中的多头注意（Multi-Head Attention）：
* K、V由编码器的输出计算得到（而不是上一个Decode Block）
* Q由上一个Decoder Block输出得到
* 好处：Decoder可以利用到Encoder的所有单词信息（单词信息无需Mask操作）

#### RLHF
TDOO
#### 幻觉
TDOO

+ LLM概念脑图{.mindmap}
    + GPT
    + transformer
    + self-attention
    + COT(Chain of Thought)
    + RLHF
    + fine-tune
    + Lora
    + Sora
    + TTS

### 参考：
* [attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
* [Transformer’s Encoder-Decoder](https://kikaben.com/transformers-encoder-decoder/)
* [Transformer’s Self-Attention](https://kikaben.com/transformers-self-attention/)
* [Transformer’s Positional Encoding](https://kikaben.com/transformers-positional-encoding/)
