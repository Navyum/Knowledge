---
title: 02.GPT
date: 2024-11-12 16:47:40
author: Navyum
tags: 
 - LLM、GPT
categories: 
 - 笔记
---

## GPT：
* Generative Pre-Trained Transformer即生成式预训练转换器，其架构基于原始的 transformer 的**解码器**

## GPT主要训练阶段：

### 无监督预训练PT（Unsupervised Pre-training）：
定义：在未标记的文本上预训练 GPT，从而利用丰富的文本语料库。该阶段又叫做生成式预训练。
任务：训练模型以了解语言的结构并捕获文本数据集中存在的统计模式。它不是针对特定的语言任务，而是<span style="color: rgb(255, 41, 65);">提高模型对语言本身的理解</span>。
具体：无监督预训练将一系列标记提供给模型（Transformer 解码器的变体）以预测下一个标记的概率。它在下图中显示为 “Text Prediction” （其中“Task Classifier” 用于监督微调SFT阶段）
![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/026b670dec056d8526150d6774badceb.png)


### 监督式微调SFT（Supervised Fine-tuning）：
使用标记数据为每个特定任务微调预训练模型，又叫做判别性微调。

#### 简单分类任务：
对于简单的分类任务，每个输入都由一系列标记和一个标签组成。它们将输入传递给预先训练的模型以获得最终的激活值，这些激活值被馈送到额外的线性 （+softmax） 输出层中

#### 文本蕴涵任务：
将一对文本之间的关系进行分类。在给定前提（文本）的情况下推断假设另一个（文本）是正确的，那么这种关系就被归类为蕴涵。如果两者之间存在不一致，则关系被归类为矛盾。如果两者都不是真的，则关系被归类为中性。

#### 相似度任务：
比较的两个句子之间没有方向关系（余弦法）

#### 问答和常识推理任务
这些任务提供了一个上下文文档$z$ 、一个问题$q$和一组可能的答案${a_k}$。对于每个可能的答案，都有一个输入序列。


## GPT-2：
* GPT-2 是 GPT-1 的直接扩展，具有更多参数并在更多数据上进行训练（1.5B）
  | 差异 | GPT-1 | GPT-2| GPT-3
  | :-- | :-- | :-- | :-- |
  | 解码器个数 | 12 | 48 | 96 |
  | 参数大小 | 0.15B | 1.5B | 17.5B |
  | 向量维度 | 512 | 1600 | 12288 |

* 在GPT-1的实验中，在没有监督微调的情况下，语言模型已经包含执行特定任务所需的信息。即所有这些知识都存储在网络参数（权重和偏差）中。
* 因此，更多的参数应该会增加语言模型的容量，**微调只是为特定任务的模型添加了最后的润色**，因此使 GPT出色的主要因素是预训练

## GPT-3：
GPT-3 显示了纯粹通过文本交互处理任务的改进能力。这些任务包括零样本（Zero-shot）、单次（one-shot）和少量样本（few-shot）示例的学习，并且必须在没有额外训练的情况下（不使用微调）执行任务。

### 元学习：
* 定义：能够学习根据过去的经验进行组合预测；学会了使用之前获得的技能来理解和实现所需的新任务

#### 上下文学习（情境学习）：
![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/5053a72bfd83e3cd7408677d1ee5cbab.png)
* 外循环：获得语言技能的无监督预训练
* 内循环：模型从示例序列中学习上下文，以预测接下来会发生什么
* 从GPT-2到GPT-3，随着参数规模的变化，情境学习才获得实质的突破

#### Zero-Shot：
不允许演示，只提供自然语言的指导

#### One-Shot：
只允许一个演示

#### Few-Shot：
学习允许尽可能多的演示（通常为 10 到 100 个）
![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/7f3ff204a9477f831a32fded2f10d10c.png){width="50%"}

![Img](https://raw.staticdn.net/Navyum/imgbed/pic/IMG/962dd7a9f23f11285c539e784d5f4d98.png)
“ppl”代表困惑，越低越好，“acc”代表准确度，越高越好
具有大容量的预训练语言模型可以通过自然语言指令和演示来利用其语言能力来理解和完成所需的任务

## 参考：
[Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
